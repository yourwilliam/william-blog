---
layout: post
title: Openstack 虚拟化优化方向
description: "Openstack虚拟化差异化优化方向"
modified: 2014-08-26
category: articles
tags: [Openstack]
comments: true
share: true
---

### 虚拟机亲和性调度机制
用于部分应用需要和某些指定应用隔离的场景，如主备场景等。

### 应用感知的大页表技术
用于高性能场景，使用大页表技术提供更高的内存hit率。

### 智能网络包中断合并技术

Linux内核在性能方面已经经历了很长一段时间的考验，尤其是2.6/3.x内核。然而，在高IO，尤其是网络方面的情况下，对中断的处理可能成为问题。我们已经在拥有一个或多个饱和1Gbps网卡的高性能系统上发现过这个问题，近来在有许多小包并发（大约10000packets/second）超载的虚拟机上也发现了这个问题。

原因很清楚：在最简单的模式中，内核通过硬件中断的方式来处理每个来自于网卡的包。但是随着数据包速率的增长，带来的中断渐渐超过了单个cpu可处理的范围。单cpu概念很重要，系统管理员对此往往认识不足。在一个普通的4-16核的系统中，因为整体cpu的使用率在6-25%左右并且系统看上去很正常，所以一个过载的内核很难被发现，。但是系统将运行很慢，并且会在没有告警，没有dmesg日志，没有明显征兆的情况下严重丢包。

详细内容请参考[技术项目 - Linux网卡中断使单个CPU过载](http://blog.csdn.net/chinanetcloud/article/details/8072455)

### 减少VM－Exit上下文等虚拟化性能敏感的指令的优化：
减少虚拟化开销，让cpu更多的做业务相关的事情

### 亲和性感知的调度，HOST/Guest NUMA 的配置调优
NUMA系统的结点通常是由一组CPU（如，SGI Altix 3000是2个Itanium2 CPU）和本地内存组成，有的结点可能还有I/O子系统。由于每个结点都有自己的本地内存，因此全系统的内存在物理上是分布的，每个结点访问本地内存和访问其它结点的远地内存的延迟是不同的，为了减少非一致性访存对系统的影响，在硬件设计时应尽量降低远地内存访存延迟（如通过Cache一致性设计等），而操作系统也必须能感知硬件的拓扑结构，优化系统的访存。

可以参考 [Linux 的 NUMA 技术](http://www.ibm.com/developerworks/cn/linux/l-numa/index.html)

### NetMap/DPDK软直通，及SR-IOV／VMDq硬直通，实现转发能力5－10倍以上的提升。

### 轻量化的Linux容器技术： 
LXC 完全不使用硬件虚拟化，在host里面使用资源隔离技术，而不是做虚拟机级别的重隔离。

Linux Containers（LXC）一种操作系统层虚拟化（Operating system–level virtualization）技术，是Linux内核容器功能的一个用户空间接口。它将应用软件系统打包成一个软件容器（Container），内含应用软件本身的代码，以及所需要的操作系统核心和库。通过统一的命名空间和共用API来分配不同软件容器的可用硬件资源，创造出应用程序的独立沙箱运行环境，使得Linux用户可以容易的创建和管理系统或应用容器。[1]

LXC不需要Hypervisor这个软件层，软件容器（Container）本身极为轻量化，提升了创建虚拟机的速度。

LXC是所谓的操作系统层次的虚拟化技术，与传统的HAL（硬件抽象层）层次的虚拟化技术相比有以下优势：

更小的虚拟化开销（LXC的诸多特性基本由内核特供，而内核实现这些特性只有极少的花费，具体分析有时间再说）
快速部署。利用LXC来隔离特定应用，只需要安装LXC，即可使用LXC相关命令来创建并启动容器来为应用提供虚拟执行环境。传统的虚拟化技术则需要先创建虚拟机，然后安装系统，再部署应用。
LXC跟其他操作系统层次的虚拟化技术相比，最大的优势在于LXC被整合进内核，不用单独为内核打补丁。

LXC技术可以参考：[LXC：Linux 容器工具](http://www.ibm.com/developerworks/cn/linux/l-lxc-containers/)
